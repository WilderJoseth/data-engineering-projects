{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Bronze layer\n",
        "\n",
        "The goal of this notebook is to perform a bronze-layer process, where raw data (csv) is saved as delta table with minimal tranformation. The idea is to store bronze data using delta format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1. Notebook parameters\n",
        "\n",
        "Parameters, which are called by pipelines.\n",
        "\n",
        "Inputs:\n",
        "\n",
        "* **in_parameter_run_id**: run identifier.\n",
        "* **in_parameter_process_date**: process date.\n",
        "* **in_parameter_path_storage**: path project storage.\n",
        "* **in_parameter_path_container**: path project container.\n",
        "* **in_parameter_bd**: bd name.\n",
        "\n",
        "Outputs:\n",
        "\n",
        "* **out_parameter_count_processed**: count of processed rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "in_parameter_run_id = 0\n",
        "in_parameter_process_date = \"1900-01-01 00:00:00\"\n",
        "in_parameter_path_storage = \"datalake20251021\"\n",
        "in_parameter_path_container = \"dajobcanada\"\n",
        "in_parameter_bd = \"dajobcanada_db\"\n",
        "out_parameter_count_processed = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2. Read raw data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import col, to_timestamp, lit, date_format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.load(f\"abfss://{in_parameter_path_container}@{in_parameter_path_storage}.dfs.core.windows.net/raw/*.csv\",\n",
        "                    format = \"csv\",\n",
        "                    header = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3. Basic transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Columns receive better name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df.select(col(\"Job ID\").alias(\"job_id\"),\n",
        "                col(\"Job Title\").alias(\"job_title\"),\n",
        "                col(\"Company Name\").alias(\"company_name\"),\n",
        "                col(\"Language and Tools\").alias(\"language_tools\"),\n",
        "                col(\"Job Salary\").alias(\"job_salary\"),\n",
        "                col(\"City\").alias(\"city\"),\n",
        "                col(\"Province\").alias(\"province\"),\n",
        "                col(\"Job Link\").alias(\"job_link\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Each load receive its own identifier (run_id), process datetime and process data for partition. Each load is stored in a partition defined by the day of process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df.withColumn(\"run_id\", lit(in_parameter_run_id))\n",
        "df = df.withColumn(\"process_datetime\", lit(in_parameter_process_date))\n",
        "df = df.withColumn(\"process_datetime\", to_timestamp(\"process_datetime\", \"yyyy-MM-dd HH:mm:ss\"))\n",
        "df = df.withColumn(\"process_date\", date_format(\"process_datetime\", \"yyyy-MM-dd\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 4. Save results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Save bronze table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {},
      "source": [
        "df.write.partitionBy(\"process_date\") \\\n",
        "                .mode(\"append\") \\\n",
        "                .format(\"delta\") \\\n",
        "                .save(f\"abfss://{in_parameter_path_container}@{in_parameter_path_storage}.dfs.core.windows.net/bronze/jobs/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Identify the number of processed rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {},
      "source": [
        "out_parameter_count_processed = df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {},
      "source": [
        "mssparkutils.notebook.exit(out_parameter_count_processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}