{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Silver layer\n",
        "\n",
        "The goal of this notebook is to perform a silver-layer process, where bronze data is read to apply cleaning transformation and the result is stored into a silver repository using delta format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1. Notebook parameters\n",
        "\n",
        "Parameters, which are called by pipelines.\n",
        "\n",
        "Inputs:\n",
        "\n",
        "* **in_parameter_run_id**: run identifier.\n",
        "* **in_parameter_process_date**: process date.\n",
        "* **in_parameter_path_storage**: path project storage.\n",
        "* **in_parameter_path_container**: path project container.\n",
        "* **in_parameter_bd**: bd name.\n",
        "\n",
        "Outputs:\n",
        "\n",
        "* **out_parameter_count_processed**: count of processed rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "in_parameter_run_id = 0\n",
        "in_parameter_process_date = \"1900-01-01 00:00:00\"\n",
        "in_parameter_path_storage = \"datalake20251021\"\n",
        "in_parameter_path_container = \"dajobcanada\"\n",
        "in_parameter_bd = \"dajobcanada_db\"\n",
        "out_parameter_count_processed = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2. Load control tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Load **validation_definition** table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "validation_definition_schema = StructType([\n",
        "    StructField(\"column_name\", StringType(), False),\n",
        "    StructField(\"column_type\", StringType(), False),\n",
        "    StructField(\"column_size\", IntegerType(), False),\n",
        "    StructField(\"column_scale\", IntegerType(), True),\n",
        "    StructField(\"default_value\", StringType(), False)\n",
        "    ])\n",
        "\n",
        "df_validation_definition = spark.read.load(f\"abfss://{in_parameter_path_container}@{in_parameter_path_storage}.dfs.core.windows.net/control_db/validation_definition/{in_parameter_run_id}.parquet\",\n",
        "                                            format = \"parquet\",\n",
        "                                            schema = validation_definition_schema,\n",
        "                                            header = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Load **job_roles** table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "job_roles_schema = StructType([\n",
        "    StructField(\"level\", StringType(), False),\n",
        "    StructField(\"role\", StringType(), False),\n",
        "    StructField(\"job_salary_min_base\", DecimalType(15, 2), False),\n",
        "    StructField(\"job_salary_max_base\", DecimalType(15, 2), False)\n",
        "    ])\n",
        "\n",
        "df_job_roles = spark.read.load(f\"abfss://{in_parameter_path_container}@{in_parameter_path_storage}.dfs.core.windows.net/control_db/job_roles/{in_parameter_run_id}.parquet\",\n",
        "                                format = \"parquet\",\n",
        "                                schema = job_roles_schema,\n",
        "                                header = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3. Read bronze data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Filter data by date of process (partition filtering)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "df = spark.read.load(f\"abfss://{in_parameter_path_container}@{in_parameter_path_storage}.dfs.core.windows.net/bronze/jobs/process_date={in_parameter_process_date[:10]}/*.parquet\",\n",
        "                    format = \"parquet\",\n",
        "                    header = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Filter data by id of process (in case there are several runs in one day)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df.filter(col(\"run_id\") == in_parameter_run_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 4. Cleaning and transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "log_schema = StructType([\n",
        "    StructField(\"id_run\", ShortType(), False),\n",
        "    StructField(\"description\", StringType(), False)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 4.1 Uppercase\n",
        "\n",
        "To standarize text and avoid any mismatch in join and filtering. I'm skipping semantic meaning for this project. The exception is \"link job\", where it is important to preserve the original text (semantic meaning)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df.withColumn(\"job_title\", upper(df[\"job_title\"]))\n",
        "df = df.withColumn(\"company_name\", upper(df[\"company_name\"]))\n",
        "df = df.withColumn(\"language_tools\", upper(df[\"language_tools\"]))\n",
        "df = df.withColumn(\"city\", upper(df[\"city\"]))\n",
        "df = df.withColumn(\"province\", upper(df[\"province\"]))\n",
        "df = df.withColumn(\"job_salary\", upper(df[\"job_salary\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 4.2. Trimming\n",
        "\n",
        "To standarize text and avoid any mismatch in join and filtering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df.withColumn(\"job_title\", trim(df[\"job_title\"]))\n",
        "df = df.withColumn(\"company_name\", trim(df[\"company_name\"]))\n",
        "df = df.withColumn(\"language_tools\", trim(df[\"language_tools\"]))\n",
        "df = df.withColumn(\"job_salary\", trim(df[\"job_salary\"]))\n",
        "df = df.withColumn(\"city\", trim(df[\"city\"]))\n",
        "df = df.withColumn(\"province\", trim(df[\"province\"]))\n",
        "df = df.withColumn(\"job_salary\", trim(df[\"job_salary\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 4.3. Fill missing values\n",
        "\n",
        "The word \"UNKNOWN\" is the standard value for missing values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 4.3.1. Identify missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {},
      "source": [
        "job_title_count_missing = df.filter(col(\"job_title\").isNull()).count()\n",
        "company_name_count_missing = df.filter(col(\"company_name\").isNull()).count()\n",
        "language_tools_count_missing = df.filter(col(\"language_tools\").isNull()).count()\n",
        "city_count_missing = df.filter(col(\"city\").isNull()).count()\n",
        "province_count_missing = df.filter(col(\"province\").isNull()).count()\n",
        "job_salary_count_missing = df.filter(col(\"job_salary\").isNull()).count()\n",
        "job_link_count_missing = df.filter(col(\"job_link\").isNull()).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "data = [(in_parameter_run_id, \"job_title missing values count: \" + str(job_title_count_missing)), \n",
        "        (in_parameter_run_id, \"company_name missing values count: \" + str(company_name_count_missing)),\n",
        "        (in_parameter_run_id, \"language_tools missing values count: \" + str(language_tools_count_missing)),\n",
        "        (in_parameter_run_id, \"city missing values count: \" + str(city_count_missing)),\n",
        "        (in_parameter_run_id, \"province missing values count: \" + str(province_count_missing)),\n",
        "        (in_parameter_run_id, \"job_salary missing values count: \" + str(job_salary_count_missing)),\n",
        "        (in_parameter_run_id, \"job_link missing values count: \" + str(job_link_count_missing)),\n",
        "        ]\n",
        "df_logs_missing_values = spark.createDataFrame(data, log_schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 4.3.2. Fill missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {},
      "source": [
        "job_title_default_v = df_validation_definition.filter(col(\"column_name\") == \"job_title\").select(\"default_value\").first()[0]\n",
        "company_name_default_v = df_validation_definition.filter(col(\"column_name\") == \"company_name\").select(\"default_value\").first()[0]\n",
        "language_tools_default_v = df_validation_definition.filter(col(\"column_name\") == \"language_tools\").select(\"default_value\").first()[0]\n",
        "city_default_v = df_validation_definition.filter(col(\"column_name\") == \"city\").select(\"default_value\").first()[0]\n",
        "province_default_v = df_validation_definition.filter(col(\"column_name\") == \"province\").select(\"default_value\").first()[0]\n",
        "job_salary_default_v = df_validation_definition.filter(col(\"column_name\") == \"job_salary\").select(\"default_value\").first()[0]\n",
        "job_link_default_v = df_validation_definition.filter(col(\"column_name\") == \"job_link\").select(\"default_value\").first()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df.fillna({\"job_title\": job_title_default_v})\n",
        "df = df.fillna({\"company_name\": company_name_default_v})\n",
        "df = df.fillna({\"language_tools\": language_tools_default_v})\n",
        "df = df.fillna({\"city\": city_default_v})\n",
        "df = df.fillna({\"province\": province_default_v})\n",
        "df = df.fillna({\"job_salary\": job_salary_default_v})\n",
        "df = df.fillna({\"job_link\": job_link_default_v})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 4.4. Remove special Characters\n",
        "\n",
        "To standarize text and avoid any mismatch in join and filtering. The exception is \"link job\", where it is important to preserve the original text (semantic meaning)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import regexp_replace\n",
        "\n",
        "pattern = \"[^a-zA-Z0-9\\\\s]\" # only letters and numbers\n",
        "df = df.withColumn(\"job_title\", regexp_replace(col(\"job_title\"), pattern, \"\"))\n",
        "df = df.withColumn(\"company_name\", regexp_replace(col(\"company_name\"), pattern, \"\"))\n",
        "df = df.withColumn(\"language_tools\", regexp_replace(col(\"language_tools\"), pattern, \"\"))\n",
        "df = df.withColumn(\"city\", regexp_replace(col(\"city\"), pattern, \"\"))\n",
        "df = df.withColumn(\"province\", regexp_replace(col(\"province\"), pattern, \"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 4.5. Extract numbers\n",
        "\n",
        "The job salary has text and numbers, but it's the numeric value is necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Remove \",\" to facilitate number extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df.withColumn(\"job_salary\", regexp_replace(\"job_salary\", \",\", \"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Apply regular expressions to extract numbers, where two groups were identified (min and max), due to job salary was define like a range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import regexp_extract\n",
        "\n",
        "# Define a regular expression to capture two numbers\n",
        "# (\\d+\\.?\\d*) captures one or more digits, optionally followed by a decimal point and more digits\n",
        "# The first group (index 1) will be the first number, the second group (index 2) the second number\n",
        "pattern = r\".*?(\\d+\\.?\\d*).*?(\\d+\\.?\\d*).*\"\n",
        "\n",
        "df = df.withColumn(\"job_salary_min\", regexp_extract(col(\"job_salary\"), pattern, 1))\n",
        "df = df.withColumn(\"job_salary_max\", regexp_extract(col(\"job_salary\"), pattern, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "If there is no value, \"0\" takes place."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {},
      "source": [
        "job_salary_min_default_v = df_validation_definition.filter(col(\"column_name\") == \"job_salary_min\").select(\"default_value\").first()[0]\n",
        "job_salary_max_default_v = df_validation_definition.filter(col(\"column_name\") == \"job_salary_max\").select(\"default_value\").first()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df.withColumn(\"job_salary_min\", \n",
        "                when(col(\"job_salary_min\") == \"\", lit(job_salary_min_default_v)).\n",
        "                otherwise(col(\"job_salary_min\"))\n",
        "                )\n",
        "\n",
        "df = df.withColumn(\"job_salary_max\", \n",
        "                when(col(\"job_salary_max\") == \"\", lit(job_salary_max_default_v)).\n",
        "                otherwise(col(\"job_salary_max\"))\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 4.6. Create type payment\n",
        "\n",
        "It was identified that several types of salaries take place (hourly, yearly, monthly, etc), but it's embedded in \"job_salary\" column, so a new column is created to make easier its identification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {},
      "source": [
        "job_salary_type_default_v = df_validation_definition.filter(col(\"column_name\") == \"job_salary_type\").select(\"default_value\").first()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df.withColumn(\"job_salary_type\", \n",
        "                when(col(\"job_salary\").like(\"%HOUR%\") == True, lit(\"H\")).\n",
        "                when(col(\"job_salary\").like(\"%YEAR%\") == True, lit(\"Y\")).\n",
        "                when(col(\"job_salary\").like(\"%MONTH%\") == True, lit(\"M\")).\n",
        "                otherwise(lit(job_salary_type_default_v))\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 4.7. Standarize remote position\n",
        "\n",
        "There are some jobs where can be done remotely, but some jobs provide information that is not necessary just adding noise, so they are removed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Fill with \"REMOTE\" all jobs can be done by that way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df.withColumn(\"city\", when(col(\"city\").like(\"%REMOTE%\") == True, lit(\"REMOTE\")).otherwise(col(\"city\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "In case of provinces, where jobs can be done remotely, a new category was created \"EVERYWHERE\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df.withColumn(\"province\", when(col(\"city\").like(\"%REMOTE%\") == True, lit(\"EVERYWHERE\")).otherwise(col(\"city\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 4.8. Create experience level\n",
        "\n",
        "A new feature is created to identify the experience required by one specific job position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {},
      "source": [
        "level_default_v = df_validation_definition.filter(col(\"column_name\") == \"level\").select(\"default_value\").first()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df.withColumn(\"level\", \n",
        "                    when((col(\"job_title\").like(\"%SR%\") == True) | (col(\"job_title\").like(\"%SENIOR%\") == True), lit(\"SENIOR\")).\n",
        "                    when(col(\"job_title\").like(\"%LEAD%\") == True, lit(\"LEAD\")).\n",
        "                    when(col(\"job_title\").like(\"%SPECIALIST%\") == True, lit(\"SPECIALIST\")).\n",
        "                    otherwise(lit(level_default_v)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 4.9. Create role\n",
        "\n",
        "A new feature is created to identify the role required by one specific job position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {},
      "source": [
        "role_default_v = df_validation_definition.filter(col(\"column_name\") == \"role\").select(\"default_value\").first()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df.withColumn(\"role\", \n",
        "                    when(col(\"job_title\").like(\"%ANALYST%\") == True, lit(\"ANALYST\")).\n",
        "                    when(col(\"job_title\").like(\"%DEVELOPER%\") == True, lit(\"DEVELOPER\")).\n",
        "                    when(col(\"job_title\").like(\"%ENGINEER%\") == True, lit(\"ENGINEER\")).\n",
        "                    otherwise(lit(role_default_v)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 5. Data integrity\n",
        "\n",
        "This step focus on data type validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 5.1. Numeric values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Apply cast to decimal and null evaluation to job salaries in order to know which pass validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {},
      "source": [
        "job_salary_min_size = df_validation_definition.filter(col(\"column_name\") == \"job_salary_min\").select(\"column_size\", \"column_scale\").first()\n",
        "job_salary_max_size = df_validation_definition.filter(col(\"column_name\") == \"job_salary_max\").select(\"column_size\", \"column_scale\").first()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df.withColumn(\"job_salary_min_is_number\", col(\"job_salary_min\").cast(DecimalType(job_salary_min_size[0], job_salary_min_size[1])).isNotNull())\n",
        "df = df.withColumn(\"job_salary_max_is_number\", col(\"job_salary_max\").cast(DecimalType(job_salary_max_size[0], job_salary_max_size[1])).isNotNull())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Filter cases that do not pass validation (log)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {},
      "source": [
        "job_salary_min_no_number = df.filter(col(\"job_salary_min_is_number\") == False).count()\n",
        "job_salary_max_no_number = df.filter(col(\"job_salary_max_is_number\") == False).count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Filter only cases that pass validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df.filter((col(\"job_salary_min_is_number\") == True) & (col(\"job_salary_max_is_number\") == True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Cast data types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df.withColumn(\"job_salary_min\", col(\"job_salary_min\").cast(DecimalType(job_salary_min_size[0], job_salary_min_size[1])))\n",
        "df = df.withColumn(\"job_salary_max\", col(\"job_salary_max\").cast(DecimalType(job_salary_max_size[0], job_salary_max_size[1])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 5.2. String values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 5.2.1. Identify length no valid\n",
        "\n",
        "To validate column sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "metadata": {},
      "source": [
        "job_title_size = df_validation_definition.filter(col(\"column_name\") == \"job_title\").select(\"column_size\").first()[0]\n",
        "company_name_size = df_validation_definition.filter(col(\"column_name\") == \"company_name\").select(\"column_size\").first()[0]\n",
        "language_tools_size = df_validation_definition.filter(col(\"column_name\") == \"language_tools\").select(\"column_size\").first()[0]\n",
        "job_salary_size = df_validation_definition.filter(col(\"column_name\") == \"job_salary\").select(\"column_size\").first()[0]\n",
        "city_size = df_validation_definition.filter(col(\"column_name\") == \"city\").select(\"column_size\").first()[0]\n",
        "province_size = df_validation_definition.filter(col(\"column_name\") == \"province\").select(\"column_size\").first()[0]\n",
        "job_link_size = df_validation_definition.filter(col(\"column_name\") == \"job_link\").select(\"column_size\").first()[0]\n",
        "level_size = df_validation_definition.filter(col(\"column_name\") == \"level\").select(\"column_size\").first()[0]\n",
        "role_size = df_validation_definition.filter(col(\"column_name\") == \"role\").select(\"column_size\").first()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df.withColumn(\"job_title_length\", length(col(\"job_title\")))\n",
        "df = df.withColumn(\"company_name_length\", length(col(\"company_name\")))\n",
        "df = df.withColumn(\"language_tools_length\", length(col(\"language_tools\")))\n",
        "df = df.withColumn(\"city_length\", length(col(\"city\")))\n",
        "df = df.withColumn(\"province_length\", length(col(\"province\")))\n",
        "df = df.withColumn(\"job_link_length\", length(col(\"job_link\")))\n",
        "df = df.withColumn(\"level_length\", length(col(\"level\")))\n",
        "df = df.withColumn(\"role_length\", length(col(\"role\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "metadata": {},
      "source": [
        "job_title_length_no_valid = df.filter(col(\"job_title_length\") > job_title_size).count()\n",
        "company_name_length_no_valid = df.filter(col(\"company_name_length\") > company_name_size).count()\n",
        "language_tools_length_no_valid = df.filter(col(\"language_tools_length\") > language_tools_size).count()\n",
        "city_length_no_valid = df.filter(col(\"city_length\") > city_size).count()\n",
        "province_length_no_valid = df.filter(col(\"province_length\") > province_size).count()\n",
        "job_link_length_no_valid = df.filter(col(\"job_link_length\") > job_link_size).count()\n",
        "level_length_no_valid = df.filter(col(\"level_length\") > level_size).count()\n",
        "role_length_no_valid = df.filter(col(\"role_length\") > role_size).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "metadata": {},
      "source": [
        "data = [(in_parameter_run_id, \"job_salary_min no number count: \" + str(job_salary_min_no_number)), \n",
        "        (in_parameter_run_id, \"job_salary_max no number count: \" + str(job_salary_max_no_number)),\n",
        "        (in_parameter_run_id, \"job_title length no valid count: \" + str(job_title_length_no_valid)),\n",
        "        (in_parameter_run_id, \"city_length length no valid count: \" + str(city_length_no_valid)),\n",
        "        (in_parameter_run_id, \"province length no valid count: \" + str(province_length_no_valid)),\n",
        "        (in_parameter_run_id, \"job_link length no valid count: \" + str(job_link_length_no_valid)),\n",
        "        (in_parameter_run_id, \"level length no valid count: \" + str(level_length_no_valid)),\n",
        "        (in_parameter_run_id, \"role length no valid count: \" + str(role_length_no_valid))\n",
        "        ]\n",
        "df_logs_data_integrity = spark.createDataFrame(data, log_schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 5.2.2. Select length valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df.filter((col(\"job_title_length\") <= job_title_size) & \\\n",
        "                (col(\"company_name_length\") <= company_name_size) & \\\n",
        "                (col(\"language_tools_length\") <= language_tools_size) & \\\n",
        "                (col(\"city_length\") <= city_size) & \\\n",
        "                (col(\"province_length\") <= province_size) & \\\n",
        "                (col(\"job_link_length\") <= job_link_size) & \\\n",
        "                (col(\"level_length\") <= level_size) & \\\n",
        "                (col(\"role_length\") <= role_size)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 6. Fill job salary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df.join(df_job_roles, on = [\"level\", \"role\"], how = \"inner\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df.withColumn(\"job_salary_min\", \n",
        "                    when(col(\"job_salary_min\") == 0, col(\"job_salary_min_base\")).\n",
        "                    otherwise(col(\"job_salary_min\")))\n",
        "\n",
        "df = df.withColumn(\"job_salary_max\", \n",
        "                    when(col(\"job_salary_max\") == 0, col(\"job_salary_max_base\")).\n",
        "                    otherwise(col(\"job_salary_max\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 7. Save log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "df_logs = df_logs_missing_values.unionAll(df_logs_data_integrity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_logs.write.mode(\"overwrite\") \\\n",
        "            .format(\"parquet\") \\\n",
        "            .save(f\"abfss://{in_parameter_path_container}@{in_parameter_path_storage}.dfs.core.windows.net/control_db/log_validation/{in_parameter_run_id}/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 8. Save results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Select final columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df.select(\"job_id\", \"job_title\", \"company_name\", \"language_tools\", \"city\", \"province\", \"job_link\",\n",
        "                \"level\",  \"role\", \"job_salary_min\", \"job_salary_max\", \"job_salary_type\", \"run_id\", \"process_datetime\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Save silver table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df.withColumn(\"process_date\", lit(in_parameter_process_date[:10]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "outputs": [],
      "metadata": {},
      "source": [
        "df.write.partitionBy(\"process_date\") \\\n",
        "                .mode(\"append\") \\\n",
        "                .format(\"delta\") \\\n",
        "                .save(f\"abfss://{in_parameter_path_container}@{in_parameter_path_storage}.dfs.core.windows.net/silver/jobs/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Identify the number of processed rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "outputs": [],
      "metadata": {},
      "source": [
        "out_parameter_count_processed = df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "outputs": [],
      "metadata": {},
      "source": [
        "mssparkutils.notebook.exit(out_parameter_count_processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}