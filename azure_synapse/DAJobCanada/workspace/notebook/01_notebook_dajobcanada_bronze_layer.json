{
	"name": "01_notebook_dajobcanada_bronze_layer",
	"properties": {
		"folder": {
			"name": "DAJobCanada"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "605e5797-3cda-4423-8832-32aa2d2d73b4"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/c4553fc9-8dc4-4718-9123-b6b30fc11ab1/resourceGroups/rg-synapze-20251021/providers/Microsoft.Synapse/workspaces/ws20251021/bigDataPools/sparkpool",
				"name": "sparkpool",
				"type": "Spark",
				"endpoint": "https://ws20251021.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.5",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Bronze layer\n",
					"\n",
					"The goal of this notebook is to perform a bronze-layer process, where raw data (csv) is saved as delta table with minimal tranformation. The idea is to store bronze data using delta format."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## 1. Notebook parameters\n",
					"\n",
					"Parameters, which are called by pipelines.\n",
					"\n",
					"Inputs:\n",
					"\n",
					"* **in_parameter_run_id**: run identifier.\n",
					"* **in_parameter_process_date**: process date.\n",
					"* **in_parameter_path_storage**: path project storage.\n",
					"* **in_parameter_path_container**: path project container.\n",
					"* **in_parameter_bd**: bd name.\n",
					"\n",
					"Outputs:\n",
					"\n",
					"* **out_parameter_count_processed**: count of processed rows."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"tags": [
						"parameters"
					]
				},
				"source": [
					"in_parameter_run_id = 0\n",
					"in_parameter_process_date = \"1900-01-01 00:00:00\"\n",
					"in_parameter_path_storage = \"datalake20251021\"\n",
					"in_parameter_path_container = \"dajobcanada\"\n",
					"in_parameter_bd = \"dajobcanada_db\"\n",
					"out_parameter_count_processed = 0"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## 2. Read raw data"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import col, to_timestamp, lit, date_format"
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"source": [
					"df = spark.read.load(f\"abfss://{in_parameter_path_container}@{in_parameter_path_storage}.dfs.core.windows.net/raw/*.csv\",\n",
					"                    format = \"csv\",\n",
					"                    header = True)"
				],
				"execution_count": 23
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## 3. Basic transformations"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Columns receive better name."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"df = df.select(col(\"Job ID\").alias(\"job_id\"),\n",
					"                col(\"Job Title\").alias(\"job_title\"),\n",
					"                col(\"Company Name\").alias(\"company_name\"),\n",
					"                col(\"Language and Tools\").alias(\"language_tools\"),\n",
					"                col(\"Job Salary\").alias(\"job_salary\"),\n",
					"                col(\"City\").alias(\"city\"),\n",
					"                col(\"Province\").alias(\"province\"),\n",
					"                col(\"Job Link\").alias(\"job_link\"))"
				],
				"execution_count": 24
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Each load receive its own identifier (run_id), process datetime and process data for partition. Each load is stored in a partition defined by the day of process."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"df = df.withColumn(\"run_id\", lit(in_parameter_run_id))\n",
					"df = df.withColumn(\"process_datetime\", lit(in_parameter_process_date))\n",
					"df = df.withColumn(\"process_datetime\", to_timestamp(\"process_datetime\", \"yyyy-MM-dd HH:mm:ss\"))\n",
					"df = df.withColumn(\"process_date\", date_format(\"process_datetime\", \"yyyy-MM-dd\"))"
				],
				"execution_count": 25
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## 4. Save results"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Save bronze table."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"df.write.partitionBy(\"process_date\") \\\n",
					"                .mode(\"append\") \\\n",
					"                .format(\"delta\") \\\n",
					"                .save(f\"abfss://{in_parameter_path_container}@{in_parameter_path_storage}.dfs.core.windows.net/bronze/jobs/\")"
				],
				"execution_count": 26
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Identify the number of processed rows."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"out_parameter_count_processed = df.count()"
				],
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"source": [
					"mssparkutils.notebook.exit(out_parameter_count_processed)"
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}